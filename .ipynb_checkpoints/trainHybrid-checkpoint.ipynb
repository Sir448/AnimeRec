{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6cbff0",
   "metadata": {},
   "source": [
    "# Anime Recommendation Training\n",
    "Train embeddings for users and anime using PyTorch, with genre features and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa93c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e5f23",
   "metadata": {},
   "source": [
    "## 1. Training parameters and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7edd5f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test is a test\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 100\n",
    "EPOCHS = 20\n",
    "CHECKPOINT_PATH = f\"checkpoints/hybrid{THRESHOLD}\"\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "CHECKPOINT_INTERVAL = 30_000\n",
    "USERNAME = 'sirawesomeness'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18db5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 70/70 [00:02<00:00, 34.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_RATINGS: 127761997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total ratings\n",
    "\n",
    "threshold_folder = f\"data/pt_files{THRESHOLD}\"\n",
    "pattern = os.path.join(threshold_folder, f\"user_anime????????????_filtered{THRESHOLD}.pt\")\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# anime_counter = Counter()\n",
    "count = 0\n",
    "for file_path in tqdm(csv_files):\n",
    "    df = torch.load(file_path)\n",
    "    count += len(df['anime_idx'])\n",
    "    if not len(df):\n",
    "        print(file_path)\n",
    "print(\"TOTAL_RATINGS:\", count)\n",
    "TOTAL_RATINGS = count\n",
    "BATCH_SIZE = 1024\n",
    "TOTAL_BATCHES = TOTAL_RATINGS // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28908bd",
   "metadata": {},
   "source": [
    "## 2. Load anime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9ec2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_id_to_idx = torch.load(f\"data/pt_files{THRESHOLD}/anime_id_to_idx.pt\", weights_only=False)\n",
    "anime_genres = torch.load(f\"data/pt_files{THRESHOLD}/anime_genres.pt\")\n",
    "\n",
    "num_anime, num_genres = anime_genres.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036b06f",
   "metadata": {},
   "source": [
    "## 3. Map all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f8f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_to_idx = torch.load(f\"data/pt_files{THRESHOLD}/user_id_to_idx.pt\")\n",
    "num_users = len(user_id_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107e65d",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6255eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsPTDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, pt_files):\n",
    "        super().__init__()\n",
    "        self.pt_files = pt_files\n",
    "\n",
    "    def __iter__(self):\n",
    "        for pt_file in self.pt_files:\n",
    "            data = torch.load(pt_file)\n",
    "            for u, a, s in zip(data[\"user_idx\"], data[\"anime_idx\"], data[\"scores\"]):\n",
    "                yield u, a, s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6031b",
   "metadata": {},
   "source": [
    "## 5. Define embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1522ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridMF(nn.Module):\n",
    "    def __init__(self, num_users, num_anime, num_genres, \n",
    "                 user_dim=64, anime_dim=128, genre_proj_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, user_dim, sparse=True)\n",
    "        self.anime_emb = nn.Embedding(num_anime, anime_dim, sparse=True)\n",
    "        self.W_genre = nn.Linear(num_genres, genre_proj_dim)\n",
    "        self.project = nn.Linear(anime_dim + genre_proj_dim, user_dim)\n",
    "        \n",
    "        # Nonlinear MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(user_dim + anime_dim + genre_proj_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_idx, anime_idx, anime_genres):\n",
    "        u = self.user_emb(user_idx)\n",
    "        v = self.anime_emb(anime_idx)\n",
    "        g = self.W_genre(anime_genres[anime_idx])\n",
    "        \n",
    "        # ----- Linear path -----\n",
    "        v_combined = torch.cat([v, g], dim=1)\n",
    "        v_proj = self.project(v_combined)\n",
    "        linear_pred = (u * v_proj).sum(dim=1)\n",
    "\n",
    "        # ----- Nonlinear path -----\n",
    "        mlp_input = torch.cat([u, v, g], dim=1)\n",
    "        nonlinear_pred = self.mlp(mlp_input).squeeze(1)\n",
    "        \n",
    "        return linear_pred + nonlinear_pred\n",
    "    \n",
    "    def recommend(self, user_idx, anime_genres, top_k=10, device=\"cpu\", exclude_ids=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Repeat user_idx across all anime\n",
    "            user_tensor = torch.tensor([user_idx] * anime_genres.size(0), \n",
    "                                       dtype=torch.long, device=device)\n",
    "            anime_tensor = torch.arange(anime_genres.size(0), \n",
    "                                        dtype=torch.long, device=device)\n",
    "            \n",
    "            preds = self.forward(user_tensor, anime_tensor, anime_genres)\n",
    "            \n",
    "            if exclude_ids is not None:\n",
    "                preds[exclude_ids] = float(\"-inf\")  # mask out watched anime\n",
    "\n",
    "            # Get top-k indices\n",
    "            top_scores, top_indices = torch.topk(preds, top_k)\n",
    "        \n",
    "        return top_indices.cpu().tolist(), top_scores.cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c526cb",
   "metadata": {},
   "source": [
    "## 6. Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c2cd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HybridMF(num_users, num_anime, num_genres).to(device)\n",
    "\n",
    "sparse_params = list(model.user_emb.parameters()) + list(model.anime_emb.parameters())\n",
    "dense_params = list(model.W_genre.parameters()) + list(model.project.parameters()) + list(model.mlp.parameters())\n",
    "\n",
    "optimizer_sparse = optim.SparseAdam(sparse_params, lr=1e-2)\n",
    "optimizer_dense = optim.Adam(dense_params, lr=1e-3, weight_decay=1e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def save_checkpoint(model, optimizer_sparse, optimizer_dense, epoch, batch_in_epoch, loss, filename):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"batch_in_epoch\": batch_in_epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_sparse_state_dict\": optimizer_sparse.state_dict(),\n",
    "        \"optimizer_dense_state_dict\": optimizer_dense.state_dict(),\n",
    "        \"loss\": loss,\n",
    "    }, filename)\n",
    "\n",
    "def load_checkpoint(model, optimizer_sparse, optimizer_dense, filename, device):\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer_sparse.load_state_dict(checkpoint[\"optimizer_sparse_state_dict\"])\n",
    "    optimizer_dense.load_state_dict(checkpoint[\"optimizer_dense_state_dict\"])\n",
    "    start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "    start_batch_in_epoch = checkpoint.get(\"batch_in_epoch\", 0)\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    return start_epoch, start_batch_in_epoch, loss\n",
    "\n",
    "pt_files = sorted(glob.glob(f\"data/pt_files{THRESHOLD}/user_anime*_filtered{THRESHOLD}.pt\"))\n",
    "dataset = RatingsPTDataset(pt_files)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "anime_genres = anime_genres.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a11d0",
   "metadata": {},
   "source": [
    "## 7. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851599e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoints/hybrid100\\epoch01.pth, epoch=1, batch_in_epoch=0, prev_loss=2.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  24%|██████████████████▎                                                         | 30009/124767 [10:39<1:22:49, 19.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid100\\epoch02.pth, Checkpoint Loss: 2.493814210877816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  25%|███████████████████▎                                                          | 30970/124767 [11:09<33:47, 46.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer_sparse\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     27\u001b[0m optimizer_dense\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 28\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_idx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manime_idx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manime_genres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, score_batch)\n\u001b[0;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 25\u001b[0m, in \u001b[0;36mHybridMF.forward\u001b[1;34m(self, user_idx, anime_idx, anime_genres)\u001b[0m\n\u001b[0;32m     23\u001b[0m v_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v, g], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m v_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject(v_combined)\n\u001b[1;32m---> 25\u001b[0m linear_pred \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv_proj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# ----- Nonlinear path -----\u001b[39;00m\n\u001b[0;32m     28\u001b[0m mlp_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([u, v, g], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_epoch, start_batch_in_epoch, prev_loss = 0, 0, None\n",
    "checkpoint_files = sorted(glob.glob(os.path.join(CHECKPOINT_PATH, \"epoch*.pth\")))\n",
    "if checkpoint_files:\n",
    "    last_checkpoint = checkpoint_files[-1]\n",
    "    start_epoch, start_batch_in_epoch, prev_loss = load_checkpoint(model, optimizer_sparse, optimizer_dense, last_checkpoint, device)\n",
    "    print(f\"Resuming from {last_checkpoint}, epoch={start_epoch}, batch_in_epoch={start_batch_in_epoch}, prev_loss={prev_loss:.4f}\")\n",
    "    \n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    checkpoint_loss = 0\n",
    "    checkpoint_count = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, total=TOTAL_BATCHES, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        if epoch == start_epoch and i < start_batch_in_epoch:\n",
    "            continue\n",
    "        \n",
    "        user_idx_batch, anime_idx_batch, score_batch = batch\n",
    "        user_idx_batch = user_idx_batch.to(device)\n",
    "        anime_idx_batch = anime_idx_batch.to(device)\n",
    "        score_batch = score_batch.to(device)\n",
    "\n",
    "        optimizer_sparse.zero_grad()\n",
    "        optimizer_dense.zero_grad()\n",
    "        pred = model(user_idx_batch, anime_idx_batch, anime_genres)\n",
    "        loss = loss_fn(pred, score_batch)\n",
    "        loss.backward()\n",
    "        optimizer_sparse.step()\n",
    "        optimizer_dense.step()\n",
    "\n",
    "        total_loss += loss.item() * len(user_idx_batch)\n",
    "        count += len(user_idx_batch)\n",
    "        \n",
    "        checkpoint_loss += loss.item() * len(user_idx_batch)\n",
    "        checkpoint_count += len(user_idx_batch)\n",
    "        \n",
    "\n",
    "        # periodic checkpoint\n",
    "        if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "            ckpt_path = os.path.join(CHECKPOINT_PATH, f\"epoch{epoch+1:02d}.pth\")\n",
    "            save_checkpoint(model, optimizer_sparse, optimizer_dense, epoch, i + 1, total_loss / count, ckpt_path)\n",
    "            progress_bar.write(f\"💾 Saved checkpoint: {ckpt_path}, Checkpoint Loss: {checkpoint_loss / max(1, checkpoint_count)}\")\n",
    "            checkpoint_loss, checkpoint_count = 0, 0\n",
    "\n",
    "    # end of epoch checkpoint\n",
    "    ckpt_path = os.path.join(CHECKPOINT_PATH, f\"epoch{epoch+1:02d}.pth\")\n",
    "    save_checkpoint(model, optimizer_sparse, optimizer_dense, epoch + 1, 0, total_loss / count, ckpt_path)\n",
    "    progress_bar.write(f\"💾 Saved checkpoint: {ckpt_path}, Epoch Loss: {total_loss / count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc3da5",
   "metadata": {},
   "source": [
    "## 8. Make Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_idx': tensor([959803, 959803, 959803,  ..., 298264, 298264, 298264]), 'anime_idx': tensor([ 3116, 12587,  9123,  ...,  2995,  3843,  3901]), 'scores': tensor([8., 8., 7.,  ..., 6., 7., 6.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(file, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# dict of tensors\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Boolean mask for rows of this user\u001b[39;00m\n\u001b[0;32m     17\u001b[0m mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_idx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m user_idx\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "# Get watched animes\n",
    "\n",
    "filtered_folder = f\"data/pt_files{THRESHOLD}\"\n",
    "pattern = os.path.join(filtered_folder, f\"user_anime????????????_filtered{THRESHOLD}.pt\")\n",
    "pt_files = glob.glob(pattern)\n",
    "user_idx = int(user_id_to_idx[USERNAME]) # convert to Python int\n",
    "\n",
    "anime_indices_list = []\n",
    "scores = []\n",
    "\n",
    "for file in tqdm(pt_files):\n",
    "    data = torch.load(file, map_location='cpu')  # dict of tensors\n",
    "\n",
    "    # Boolean mask for rows of this user\n",
    "    mask = data['user_idx'] == user_idx\n",
    "\n",
    "    # Extract anime indices for this user\n",
    "    user_animes = data['anime_idx'][mask]\n",
    "    user_scores = data['scores'][mask]\n",
    "\n",
    "    if user_animes.numel() > 0:\n",
    "        anime_indices_list.extend(user_animes)\n",
    "        scores.extend(user_scores)\n",
    "        \n",
    "df = pd.read_csv('data/original/anime.csv',sep='\\t')\n",
    "df = df.iloc[anime_indices_list]\n",
    "\n",
    "for title,score in zip(df['title'],scores):\n",
    "    # print(score, title)\n",
    "    print(int(score), title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "417b0110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction from checkpoints/hybrid\\epoch20.pth, loss=1.5750\n",
      "Recommendations for killerlovers\n",
      "                                                 title  \\\n",
      "9953  Ginga Eiyuu Densetsu: Die Neue These - Gekitotsu   \n",
      "4892                        Xingguang Jiazu Tebie Pian   \n",
      "6496                            Osakini Douzo Arigatou   \n",
      "445                        Konu to Tanoshii Otomodachi   \n",
      "9429                      Pokopon no Yukai na Saiyuuki   \n",
      "4436                               Xiang Shi Chuanshuo   \n",
      "353                                         Jigokuraku   \n",
      "549                                    Ore wa Chokkaku   \n",
      "9963                                Matsugae wo Musubi   \n",
      "9862                         Xiong Chumo: Bian Xing Ji   \n",
      "\n",
      "                                              anime_url  \n",
      "9953  https://myanimelist.net/anime/42886/Ginga_Eiyu...  \n",
      "4892  https://myanimelist.net/anime/48040/Xingguang_...  \n",
      "6496  https://myanimelist.net/anime/35175/Osakini_Do...  \n",
      "445   https://myanimelist.net/anime/50142/Konu_to_Ta...  \n",
      "9429  https://myanimelist.net/anime/32621/Pokopon_no...  \n",
      "4436  https://myanimelist.net/anime/48045/Xiang_Shi_...  \n",
      "353      https://myanimelist.net/anime/46569/Jigokuraku  \n",
      "549   https://myanimelist.net/anime/19831/Ore_wa_Cho...  \n",
      "9963  https://myanimelist.net/anime/38130/Matsugae_w...  \n",
      "9862  https://myanimelist.net/anime/40439/Xiong_Chum...  \n"
     ]
    }
   ],
   "source": [
    "checkpoint_files = sorted(glob.glob(os.path.join(CHECKPOINT_PATH, \"epoch*.pth\")))\n",
    "if checkpoint_files:\n",
    "    last_checkpoint = checkpoint_files[-1]\n",
    "    _, _, loss = load_checkpoint(model, optimizer_sparse, optimizer_dense, last_checkpoint, device)\n",
    "    print(f\"Making prediction from {last_checkpoint}, loss={loss:.4f}\")\n",
    "\n",
    "user_idx = user_id_to_idx[USERNAME]\n",
    "top_indices, top_scores = model.recommend(user_idx, anime_genres, device=device,exclude_ids=[])\n",
    "df = pd.read_csv('data/original/anime.csv',sep='\\t')\n",
    "df = df.iloc[top_indices]\n",
    "print(f\"Recommendations for {USERNAME}\")\n",
    "print(df[['title','anime_url']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
