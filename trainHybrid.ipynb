{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6cbff0",
   "metadata": {},
   "source": [
    "# Anime Recommendation Training\n",
    "Train embeddings for users and anime using PyTorch, with genre features and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa93c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e5f23",
   "metadata": {},
   "source": [
    "## 1. Training parameters and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7edd5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 500\n",
    "EPOCHS = 20\n",
    "CHECKPOINT_PATH = f\"checkpoints/hybrid{THRESHOLD}\"\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "CHECKPOINT_INTERVAL = 30_000\n",
    "USERNAME = 'sirawesomeness'\n",
    "USERNAME = 'catfire8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18db5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:01<00:00, 36.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_RATINGS: 127102285\n",
      "THRESHOLD: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Count total ratings\n",
    "\n",
    "threshold_folder = f\"data/pt_files{THRESHOLD}\"\n",
    "pattern = os.path.join(threshold_folder, f\"user_anime????????????_filtered{THRESHOLD}.pt\")\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# anime_counter = Counter()\n",
    "count = 0\n",
    "for file_path in tqdm(csv_files):\n",
    "    df = torch.load(file_path)\n",
    "    count += len(df['anime_idx'])\n",
    "    if not len(df):\n",
    "        print(file_path)\n",
    "print(\"TOTAL_RATINGS:\", count)\n",
    "print(\"THRESHOLD:\",THRESHOLD)\n",
    "TOTAL_RATINGS = count\n",
    "BATCH_SIZE = 1024\n",
    "TOTAL_BATCHES = TOTAL_RATINGS // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28908bd",
   "metadata": {},
   "source": [
    "## 2. Load anime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa9ec2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF ANIMES: 7448\n",
      "NUMBER OF GENRES: 44\n"
     ]
    }
   ],
   "source": [
    "anime_id_to_idx = torch.load(f\"data/pt_files{THRESHOLD}/anime_id_to_idx.pt\", weights_only=False)\n",
    "anime_genres = torch.load(f\"data/pt_files{THRESHOLD}/anime_genres.pt\")\n",
    "\n",
    "num_anime, num_genres = anime_genres.shape\n",
    "print(\"NUMBER OF ANIMES:\",num_anime)\n",
    "print(\"NUMBER OF GENRES:\",num_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036b06f",
   "metadata": {},
   "source": [
    "## 3. Map all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51f8f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF USERS: 977946\n"
     ]
    }
   ],
   "source": [
    "user_id_to_idx = torch.load(f\"data/pt_files{THRESHOLD}/user_id_to_idx.pt\")\n",
    "num_users = len(user_id_to_idx)\n",
    "\n",
    "print(\"NUMBER OF USERS:\", num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e107e65d",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6255eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsPTDataset(torch.utils.data.IterableDataset):\n",
    "    _global_seed = None\n",
    "    \n",
    "    def __init__(self, pt_files, seed=None):\n",
    "        super().__init__()\n",
    "        self.pt_files = pt_files\n",
    "        \n",
    "        if seed:\n",
    "            RatingsPTDataset._global_seed = seed\n",
    "\n",
    "        # If no global seed exists yet, generate one\n",
    "        if RatingsPTDataset._global_seed is None:\n",
    "            RatingsPTDataset._global_seed = random.randint(0, 2**32 - 1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "\n",
    "        # Global RNG (shared seed ensures same shuffle across all workers)\n",
    "        rng = random.Random(RatingsPTDataset._global_seed)\n",
    "\n",
    "        pt_files = list(self.pt_files)\n",
    "\n",
    "        # Global shuffle (same order for all workers)\n",
    "        rng.shuffle(pt_files)\n",
    "\n",
    "        if worker_info is None:\n",
    "            assigned_files = pt_files\n",
    "        else:\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "\n",
    "            # Deterministic split, no overlap\n",
    "            files_per_worker = ceil(len(pt_files) / num_workers)\n",
    "            start = worker_id * files_per_worker\n",
    "            end = min(start + files_per_worker, len(pt_files))\n",
    "            assigned_files = pt_files[start:end]\n",
    "\n",
    "        # Each worker now has a unique set of files\n",
    "        # Local RNG for within-file shuffling\n",
    "        local_rng = random.Random(RatingsPTDataset._global_seed + (worker_id if worker_info else 0))\n",
    "\n",
    "        for pt_file in assigned_files:\n",
    "            data = torch.load(pt_file)\n",
    "            samples = list(zip(data[\"user_idx\"], data[\"anime_idx\"], data[\"scores\"]))\n",
    "\n",
    "            local_rng.shuffle(samples)\n",
    "\n",
    "            for u, a, s in samples:\n",
    "                yield u, a, s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe6031b",
   "metadata": {},
   "source": [
    "## 5. Define embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1522ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridMF(nn.Module):\n",
    "    def __init__(self, num_users, num_anime, num_genres, \n",
    "                 user_dim=64, anime_dim=128, genre_proj_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, user_dim, sparse=True)\n",
    "        self.anime_emb = nn.Embedding(num_anime, anime_dim, sparse=True)\n",
    "        self.W_genre = nn.Linear(num_genres, genre_proj_dim)\n",
    "        self.project = nn.Linear(anime_dim + genre_proj_dim, user_dim)\n",
    "        \n",
    "        # Nonlinear MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(user_dim + anime_dim + genre_proj_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, user_idx, anime_idx, anime_genres):\n",
    "        u = self.user_emb(user_idx)\n",
    "        v = self.anime_emb(anime_idx)\n",
    "        g = self.W_genre(anime_genres[anime_idx])\n",
    "        \n",
    "        # ----- Linear path -----\n",
    "        v_combined = torch.cat([v, g], dim=1)\n",
    "        v_proj = self.project(v_combined)\n",
    "        linear_pred = (u * v_proj).sum(dim=1)\n",
    "\n",
    "        # ----- Nonlinear path -----\n",
    "        mlp_input = torch.cat([u, v, g], dim=1)\n",
    "        nonlinear_pred = self.mlp(mlp_input).squeeze(1)\n",
    "        \n",
    "        return linear_pred + nonlinear_pred\n",
    "    \n",
    "    def recommend(self, user_idx, anime_genres, top_k=10, device=\"cpu\", exclude_ids=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Repeat user_idx across all anime\n",
    "            user_tensor = torch.tensor([user_idx] * anime_genres.size(0), \n",
    "                                       dtype=torch.long, device=device)\n",
    "            anime_tensor = torch.arange(anime_genres.size(0), \n",
    "                                        dtype=torch.long, device=device)\n",
    "            \n",
    "            preds = self.forward(user_tensor, anime_tensor, anime_genres)\n",
    "            \n",
    "            if exclude_ids is not None:\n",
    "                preds[exclude_ids] = float(\"-inf\")  # mask out watched anime\n",
    "\n",
    "            # Get top-k indices\n",
    "            top_scores, top_indices = torch.topk(preds, top_k)\n",
    "        \n",
    "        return top_indices.cpu().tolist(), top_scores.cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c526cb",
   "metadata": {},
   "source": [
    "## 6. Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c2cd9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HybridMF(num_users, num_anime, num_genres).to(device)\n",
    "\n",
    "sparse_params = list(model.user_emb.parameters()) + list(model.anime_emb.parameters())\n",
    "dense_params = list(model.W_genre.parameters()) + list(model.project.parameters()) + list(model.mlp.parameters())\n",
    "\n",
    "optimizer_sparse = optim.SparseAdam(sparse_params, lr=5e-3)\n",
    "optimizer_dense = optim.Adam(dense_params, lr=1e-3, weight_decay=1e-5)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def save_checkpoint(model, optimizer_sparse, optimizer_dense, epoch, batch_in_epoch, loss, filename):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"batch_in_epoch\": batch_in_epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_sparse_state_dict\": optimizer_sparse.state_dict(),\n",
    "        \"optimizer_dense_state_dict\": optimizer_dense.state_dict(),\n",
    "        \"loss\": loss,\n",
    "        \"seed\": RatingsPTDataset._global_seed,\n",
    "    }, filename)\n",
    "\n",
    "def load_checkpoint(model, optimizer_sparse, optimizer_dense, filename, device):\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer_sparse.load_state_dict(checkpoint[\"optimizer_sparse_state_dict\"])\n",
    "    optimizer_dense.load_state_dict(checkpoint[\"optimizer_dense_state_dict\"])\n",
    "    start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "    start_batch_in_epoch = checkpoint.get(\"batch_in_epoch\", 0)\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    RatingsPTDataset._global_seed = checkpoint[\"seed\"]\n",
    "    return start_epoch, start_batch_in_epoch, loss\n",
    "\n",
    "pt_files = sorted(glob.glob(f\"data/pt_files{THRESHOLD}/user_anime*_filtered{THRESHOLD}.pt\"))\n",
    "dataset = RatingsPTDataset(pt_files)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "anime_genres = anime_genres.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a11d0",
   "metadata": {},
   "source": [
    "## 7. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "851599e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoints/hybrid500\\epoch16.pth, epoch=15, batch_in_epoch=30000, prev_loss=1.6277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:  48%|████▊     | 60022/124123 [19:44<29:24, 36.34it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch16.pth, Checkpoint Loss: 1.6264327452371519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:  73%|███████▎  | 90015/124123 [31:02<16:36, 34.23it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch16.pth, Checkpoint Loss: 1.6187181806628903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:  97%|█████████▋| 120008/124123 [41:11<03:38, 18.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch16.pth, Checkpoint Loss: 1.6194991505141059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 124124it [42:55, 48.20it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch16.pth, Epoch Loss: 1.62213110350526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  24%|██▍       | 30013/124123 [12:05<1:04:15, 24.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch17.pth, Checkpoint Loss: 1.6141579028129578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  48%|████▊     | 60012/124123 [24:45<44:13, 24.16it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch17.pth, Checkpoint Loss: 1.6126426739409565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  73%|███████▎  | 90013/124123 [37:10<24:08, 23.55it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch17.pth, Checkpoint Loss: 1.6049756613428394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  97%|█████████▋| 120010/124123 [50:29<02:35, 26.46it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch17.pth, Checkpoint Loss: 1.6059218395690122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 124124it [52:14, 39.60it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch17.pth, Epoch Loss: 1.6097923115418045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  24%|██▍       | 30009/124123 [12:25<1:26:10, 18.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch18.pth, Checkpoint Loss: 1.6017880773713191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  48%|████▊     | 60017/124123 [24:39<41:47, 25.56it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch18.pth, Checkpoint Loss: 1.6005885450065136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  73%|███████▎  | 90014/124123 [35:11<23:17, 24.40it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch18.pth, Checkpoint Loss: 1.5926718141600489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  97%|█████████▋| 120013/124123 [46:39<02:27, 27.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch18.pth, Checkpoint Loss: 1.5938965509931247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 124124it [48:15, 42.87it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch18.pth, Epoch Loss: 1.5975936180401593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:  24%|██▍       | 30019/124123 [11:28<52:20, 29.96it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch19.pth, Checkpoint Loss: 1.5901327853664755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:  48%|████▊     | 60016/124123 [23:28<36:51, 28.99it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch19.pth, Checkpoint Loss: 1.5891346212198336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:  73%|███████▎  | 90016/124123 [34:33<18:21, 30.95it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch19.pth, Checkpoint Loss: 1.5814408552984396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:  97%|█████████▋| 120019/124123 [46:10<02:02, 33.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch19.pth, Checkpoint Loss: 1.5819073496490716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 124124it [47:42, 43.36it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch19.pth, Epoch Loss: 1.585927664000823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:  24%|██▍       | 30018/124123 [11:22<54:22, 28.84it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch20.pth, Checkpoint Loss: 1.5798948830217123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:  48%|████▊     | 60014/124123 [23:17<34:19, 31.13it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch20.pth, Checkpoint Loss: 1.5780280316467086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:  73%|███████▎  | 90010/124123 [34:13<20:36, 27.58it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch20.pth, Checkpoint Loss: 1.5710084089492757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:  97%|█████████▋| 120013/124123 [45:46<02:52, 23.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch20.pth, Checkpoint Loss: 1.5710629819765687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 124124it [47:20, 43.70it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved checkpoint: checkpoints/hybrid500\\epoch20.pth, Epoch Loss: 1.5752388100541495\n"
     ]
    }
   ],
   "source": [
    "start_epoch, start_batch_in_epoch, prev_loss = 0, 0, None\n",
    "checkpoint_files = sorted(glob.glob(os.path.join(CHECKPOINT_PATH, \"epoch*.pth\")))\n",
    "if checkpoint_files:\n",
    "    last_checkpoint = checkpoint_files[-1]\n",
    "    start_epoch, start_batch_in_epoch, prev_loss = load_checkpoint(model, optimizer_sparse, optimizer_dense, last_checkpoint, device)\n",
    "    print(f\"Resuming from {last_checkpoint}, epoch={start_epoch}, batch_in_epoch={start_batch_in_epoch}, prev_loss={prev_loss:.4f}\")\n",
    "    \n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    checkpoint_loss = 0\n",
    "    checkpoint_count = 0\n",
    "    \n",
    "    progress_bar = tqdm(loader, total=TOTAL_BATCHES, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        if epoch == start_epoch and i < start_batch_in_epoch:\n",
    "            continue\n",
    "        \n",
    "        user_idx_batch, anime_idx_batch, score_batch = batch\n",
    "        user_idx_batch = user_idx_batch.to(device)\n",
    "        anime_idx_batch = anime_idx_batch.to(device)\n",
    "        score_batch = score_batch.to(device)\n",
    "\n",
    "        optimizer_sparse.zero_grad()\n",
    "        optimizer_dense.zero_grad()\n",
    "        pred = model(user_idx_batch, anime_idx_batch, anime_genres)\n",
    "        loss = loss_fn(pred, score_batch)\n",
    "        loss.backward()\n",
    "        optimizer_sparse.step()\n",
    "        optimizer_dense.step()\n",
    "\n",
    "        total_loss += loss.item() * len(user_idx_batch)\n",
    "        count += len(user_idx_batch)\n",
    "        \n",
    "        checkpoint_loss += loss.item() * len(user_idx_batch)\n",
    "        checkpoint_count += len(user_idx_batch)\n",
    "        \n",
    "\n",
    "        # periodic checkpoint\n",
    "        if (i + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "            ckpt_path = os.path.join(CHECKPOINT_PATH, f\"epoch{epoch+1:02d}.pth\")\n",
    "            save_checkpoint(model, optimizer_sparse, optimizer_dense, epoch, i + 1, total_loss / count, ckpt_path)\n",
    "            progress_bar.write(f\"💾 Saved checkpoint: {ckpt_path}, Checkpoint Loss: {checkpoint_loss / max(1, checkpoint_count)}\")\n",
    "            checkpoint_loss, checkpoint_count = 0, 0\n",
    "\n",
    "    # end of epoch checkpoint\n",
    "    ckpt_path = os.path.join(CHECKPOINT_PATH, f\"epoch{epoch+1:02d}.pth\")\n",
    "    save_checkpoint(model, optimizer_sparse, optimizer_dense, epoch + 1, 0, total_loss / count, ckpt_path)\n",
    "    progress_bar.write(f\"💾 Saved checkpoint: {ckpt_path}, Epoch Loss: {total_loss / count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc3da5",
   "metadata": {},
   "source": [
    "## 8. Make Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb31a81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [00:02<00:00, 33.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime sirawesomeness rated 10 to verify\n",
      "10 Naruto: Shippuuden\n",
      "10 Naruto\n",
      "10 Fate/stay night Movie: Heaven's Feel - I. Presage Flower\n",
      "10 JoJo no Kimyou na Bouken Part 3: Stardust Crusaders 2nd Season\n",
      "10 Fate/stay night: Unlimited Blade Works 2nd Season\n",
      "10 Koe no Katachi\n",
      "10 Gintama°\n",
      "10 Code Geass: Hangyaku no Lelouch R2\n",
      "10 Re:Zero kara Hajimeru Isekai Seikatsu\n",
      "10 Fate/stay night Movie: Heaven's Feel - II. Lost Butterfly\n",
      "10 Fate/stay night Movie: Heaven's Feel - III. Spring Song\n",
      "10 Haikyuu!! Movie 4: Concept no Tatakai\n",
      "10 Boku no Hero Academia 4th Season\n",
      "10 Mushoku Tensei: Isekai Ittara Honki Dasu\n",
      "10 Re:Zero kara Hajimeru Isekai Seikatsu 2nd Season\n",
      "10 Shokugeki no Souma: Shin no Sara\n",
      "10 Shingeki no Kyojin: The Final Season\n",
      "10 BNA\n",
      "10 Haikyuu!! To the Top 2nd Season\n",
      "10 Shokugeki no Souma: Gou no Sara\n",
      "10 Mushoku Tensei: Isekai Ittara Honki Dasu Part 2\n",
      "10 Odd Taxi\n"
     ]
    }
   ],
   "source": [
    "# Get watched animes\n",
    "\n",
    "filtered_folder = f\"data/pt_files{THRESHOLD}\"\n",
    "pattern = os.path.join(filtered_folder, f\"user_anime????????????_filtered{THRESHOLD}.pt\")\n",
    "pt_files = glob.glob(pattern)\n",
    "user_idx = int(user_id_to_idx[USERNAME]) # convert to Python int\n",
    "anime_indices_list = []\n",
    "scores = []\n",
    "\n",
    "for file in tqdm(pt_files):\n",
    "    data = torch.load(file, map_location='cpu')  # dict of tensors\n",
    "\n",
    "    # Boolean mask for rows of this user\n",
    "    mask = data['user_idx'] == user_idx\n",
    "\n",
    "    # Extract anime indices for this user\n",
    "    user_animes = data['anime_idx'][mask]\n",
    "    user_scores = data['scores'][mask]\n",
    "\n",
    "    if user_animes.numel() > 0:\n",
    "        anime_indices_list.extend(user_animes)\n",
    "        scores.extend(user_scores)\n",
    "        \n",
    "anime_path = \"data/filteredRatings/anime_filtered.csv\" if not THRESHOLD else f\"data/filteredRatingsThreshold{THRESHOLD}/anime_filtered.csv\"\n",
    "df = pd.read_csv(anime_path)\n",
    "df = df.iloc[anime_indices_list]\n",
    "print(f\"Anime {USERNAME} rated 10 to verify\")\n",
    "for title,score in zip(df['title'],scores):\n",
    "    score = int(score)\n",
    "    if score == 10:\n",
    "        print(score, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "417b0110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction from checkpoints/hybrid500\\epoch20.pth, loss=1.5752\n",
      "Recommendations for catfire8\n",
      "Index   Title                                  URL\n",
      "-----------------------------------------------------------------------------\n",
      "1       Monster                                https://myanimelist.net/anime/19\n",
      "2       Ginga Eiyuu Densetsu                   https://myanimelist.net/anime/820\n",
      "3       Owarimonogatari 2nd Season             https://myanimelist.net/anime/35247\n",
      "4       Shoujo☆Kageki Revue Starlight Movie    https://myanimelist.net/anime/40664\n",
      "5       3-gatsu no Lion 2nd Season             https://myanimelist.net/anime/35180\n",
      "6       Mushishi Zoku Shou 2nd Season          https://myanimelist.net/anime/24701\n",
      "7       Mushishi                               https://myanimelist.net/anime/457\n",
      "8       Mushishi Zoku Shou: Suzu no Shizuku    https://myanimelist.net/anime/28957\n",
      "9       Ping Pong the Animation                https://myanimelist.net/anime/22135\n",
      "10      Monogatari Series: Second Season       https://myanimelist.net/anime/17074\n"
     ]
    }
   ],
   "source": [
    "checkpoint_files = sorted(glob.glob(os.path.join(CHECKPOINT_PATH, \"epoch*.pth\")))\n",
    "if checkpoint_files:\n",
    "    last_checkpoint = checkpoint_files[-1]\n",
    "    _, _, loss = load_checkpoint(model, optimizer_sparse, optimizer_dense, last_checkpoint, device)\n",
    "    print(f\"Making prediction from {last_checkpoint}, loss={loss:.4f}\")\n",
    "\n",
    "user_idx = user_id_to_idx[USERNAME]\n",
    "# top_indices, top_scores = model.recommend(user_idx, anime_genres, device=device)\n",
    "top_indices, top_scores = model.recommend(user_idx, anime_genres, device=device,exclude_ids=anime_indices_list)\n",
    "anime_path = \"data/filteredRatings/anime_filtered.csv\" if not THRESHOLD else f\"data/filteredRatingsThreshold{THRESHOLD}/anime_filtered.csv\"\n",
    "df = pd.read_csv(anime_path)\n",
    "df = df.iloc[top_indices]\n",
    "print(f\"Recommendations for {USERNAME}\")\n",
    "\n",
    "# Calculate dynamic padding based on the longest title\n",
    "max_title_len = df['title'].map(len).max()\n",
    "title_col_width = max(max_title_len, len(\"Title\")) + 2  # +2 for a little breathing room\n",
    "\n",
    "# Print formatted header\n",
    "print(f\"{'Index':<6}  {'Title':<{title_col_width}}  {'URL'}\")\n",
    "print(\"-\" * (title_col_width + 40))  # Adjust line length for aesthetics\n",
    "\n",
    "# Print rows with dynamic spacing\n",
    "index = 1\n",
    "for _, row in df.iterrows():\n",
    "    url = f\"https://myanimelist.net/anime/{row['anime_id']}\"\n",
    "    print(f\"{index:<6}  {row['title']:<{title_col_width}}  {url}\")\n",
    "    index += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "440d2a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_16388\\3798601663.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_idx_batch = torch.tensor(sample_data[\"user_idx\"][:BATCH_SIZE_CHECK])\n",
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_16388\\3798601663.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  anime_idx_batch = torch.tensor(sample_data[\"anime_idx\"][:BATCH_SIZE_CHECK])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dead ReLU neuron fraction per checkpoint:\n",
      "Epoch | % Dead Neurons\n",
      "----------------------\n",
      "  Epoch 1: 63.96% zeros\n",
      "  Epoch 2: 72.46% zeros\n",
      "  Epoch 3: 83.20% zeros\n",
      "  Epoch 4: 88.09% zeros\n",
      "  Epoch 5: 79.69% zeros\n",
      "  Epoch 6: 91.89% zeros\n",
      "  Epoch 7: 86.72% zeros\n",
      "  Epoch 8: 85.74% zeros\n",
      "  Epoch 9: 91.99% zeros\n",
      "  Epoch 10: 94.92% zeros\n",
      "  Epoch 11: 90.43% zeros\n",
      "  Epoch 12: 91.89% zeros\n",
      "  Epoch 13: 90.62% zeros\n",
      "  Epoch 14: 92.09% zeros\n",
      "  Epoch 15: 89.16% zeros\n",
      "  Epoch 16: 90.72% zeros\n",
      "  Epoch 17: 88.18% zeros\n",
      "  Epoch 18: 89.55% zeros\n",
      "  Epoch 19: 91.41% zeros\n",
      "  Epoch 20: 93.75% zeros\n"
     ]
    }
   ],
   "source": [
    "def relu_hook(name, stats_dict):\n",
    "    def hook(module, input, output):\n",
    "        zeros = (output == 0).sum().item()\n",
    "        total = output.numel()\n",
    "        stats_dict[name] = {\n",
    "            \"zero_count\": zeros,\n",
    "            \"total_count\": total,\n",
    "            \"zero_fraction\": zeros / total\n",
    "        }\n",
    "    return hook\n",
    "\n",
    "def check_dead_neurons(model, user_idx_batch, anime_idx_batch, anime_genres, device):\n",
    "    activation_stats = {}\n",
    "\n",
    "    # Register hooks on all ReLU layers\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            hooks.append(module.register_forward_hook(relu_hook(name, activation_stats)))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_idx_batch = user_idx_batch.to(device)\n",
    "        anime_idx_batch = anime_idx_batch.to(device)\n",
    "        anime_genres = anime_genres.to(device)\n",
    "        _ = model(user_idx_batch, anime_idx_batch, anime_genres)\n",
    "\n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    return activation_stats\n",
    "\n",
    "BATCH_SIZE_CHECK = 32\n",
    "sample_data = torch.load(pt_files[0], map_location='cpu')\n",
    "user_idx_batch = torch.tensor(sample_data[\"user_idx\"][:BATCH_SIZE_CHECK])\n",
    "anime_idx_batch = torch.tensor(sample_data[\"anime_idx\"][:BATCH_SIZE_CHECK])\n",
    "\n",
    "# Loop over all checkpoints\n",
    "checkpoint_files = sorted(glob.glob(os.path.join(CHECKPOINT_PATH, \"epoch*.pth\")))\n",
    "\n",
    "print(\"\\nDead ReLU neuron fraction per checkpoint:\")\n",
    "print(\"Epoch | % Dead Neurons\")\n",
    "print(\"----------------------\")\n",
    "for ckpt_file in checkpoint_files:\n",
    "    epoch = int(os.path.basename(ckpt_file).replace(\"epoch\", \"\").replace(\".pth\", \"\"))\n",
    "    load_checkpoint(model, optimizer_sparse, optimizer_dense, ckpt_file, device)\n",
    "    stats = check_dead_neurons(model, user_idx_batch, anime_idx_batch, anime_genres, device)\n",
    "\n",
    "    for layer, s in stats.items():\n",
    "        print(f\"Epoch {epoch}: {s['zero_fraction']*100:.2f}% zeros\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
